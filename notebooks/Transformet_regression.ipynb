{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f841ed1-e078-4dfd-91db-5ec2a6c1433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LayerNormalization, Dense, Dropout, MultiHeadAttention, Layer,GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "687100f2-17a6-4461-a9e7-5f7721c0bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"../data/final_features_with_targets.csv\", parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "final_df = final_df.dropna().copy()\n",
    "\n",
    "assets = ['EEM', 'Gold', 'FTSE100', 'S&P500', 'Nikkei225', 'UST10Y']\n",
    "\n",
    "#For each asset to pick pick all columns\n",
    "feature_suffixes = ['log_ret', 'vol_30d', 'zscore', 'sma5', 'macd', 'vol_spike']\n",
    "global_features = ['covid_flag']\n",
    "lookback = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25affd9b-6f63-4f5c-b48e-1f0441672297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing EEM...\n",
      "Processing Gold...\n",
      "Processing FTSE100...\n",
      "Processing S&P500...\n",
      "Processing Nikkei225...\n",
      "Processing UST10Y...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "lookback = 20\n",
    "X_seq_dict = {}\n",
    "y_seq_dict = {}\n",
    "\n",
    "for asset in assets:\n",
    "    print(f\"Processing {asset}...\")\n",
    "    \n",
    "    # Define feature and target columns\n",
    "    feature_cols = [f\"{asset}_{f}\" for f in feature_suffixes] + global_features\n",
    "    target_col = f\"{asset}_target_5d\"\n",
    "\n",
    "    # Features and target extraction \n",
    "    data = final_df[feature_cols + [target_col]].copy()\n",
    "\n",
    "    # Chronological split\n",
    "    split_idx = int(0.8 * len(data))\n",
    "    train_data = data.iloc[:split_idx]\n",
    "    test_data = data.iloc[split_idx:]\n",
    "\n",
    "  \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data[feature_cols].values)\n",
    "    X_test_scaled = scaler.transform(test_data[feature_cols].values)\n",
    "\n",
    "    \n",
    "    y_train = train_data[target_col].values\n",
    "    y_test = test_data[target_col].values\n",
    "\n",
    "    # Function to create sequences \n",
    "    def create_sequences(X, y, lookback):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(lookback, len(X)):\n",
    "            Xs.append(X[i - lookback:i])\n",
    "            ys.append(y[i])\n",
    "        return np.array(Xs), np.array(ys)\n",
    "\n",
    "    # Sequences for both train and test\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, lookback)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, lookback)\n",
    "\n",
    "    # Save to dictionary\n",
    "    X_seq_dict[asset] = (X_train_seq, X_test_seq)\n",
    "    y_seq_dict[asset] = (y_train_seq, y_test_seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cfeebd5-2954-4970-828f-d39bf1005ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold →\n",
      "X_train_seq shape: (2739, 20, 7)\n",
      "y_train_seq shape: (2739,)\n",
      "X_test_seq shape:  (670, 20, 7)\n",
      "y_test_seq shape:  (670,)\n"
     ]
    }
   ],
   "source": [
    "#Testing the EEM data as a sample\n",
    "asset_to_check = 'Gold'\n",
    "\n",
    "X_train_seq, X_test_seq = X_seq_dict[asset_to_check]\n",
    "y_train_seq, y_test_seq = y_seq_dict[asset_to_check]\n",
    "\n",
    "print(f\"{asset_to_check} →\")\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"X_test_seq shape:  {X_test_seq.shape}\")\n",
    "print(f\"y_test_seq shape:  {y_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c049f087-23ea-4ad2-b1e3-bac97cced474",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20           # lookback already defined but name change for easier understanding\n",
    "n_features = 7         # All the features we have creared initially \n",
    "embed_dim = 128          # Dimention embedding for each step \n",
    "num_heads = 8          # Multi head attention, can change this based on results \n",
    "ff_dim = 256           # Feedforward layer size inside Transformer block\n",
    "dropout_rate = 0.05     # Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e75aff0e-98b0-41a4-952b-8fd02b4fa3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(seq_len, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cae66668-1679-4b4d-b373-ef714259bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.05):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48511086-3a2b-4103-80c4-cc2ac8fc63dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_block_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">593,920</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_block_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">593,920</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_block_6     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">593,920</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_block_7     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">593,920</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_block_8     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">593,920</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_block_9     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">593,920</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m7\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_block_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m593,920\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_block_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m593,920\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_block_6     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m593,920\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_block_7     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m593,920\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_block_8     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m593,920\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_block_9     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m593,920\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_33 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,572,865</span> (13.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,572,865\u001b[0m (13.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,572,865</span> (13.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,572,865\u001b[0m (13.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def transformer_regressor(seq_len, n_features, embed_dim=128, ff_dim=256, num_heads=8):\n",
    "    inputs = Input(shape=(seq_len, n_features))\n",
    "    x = Dense(embed_dim)(inputs)\n",
    "\n",
    "    # --- Use your best architecture (e.g., stacked blocks) ---\n",
    "    num_encoder_blocks = 6\n",
    "    for _ in range(num_encoder_blocks):\n",
    "        encoder_block = TransformerEncoderBlock(embed_dim=embed_dim, ff_dim=ff_dim, num_heads=num_heads)\n",
    "        x = encoder_block(x)\n",
    "    # ---\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "    \n",
    "    outputs = Dense(1, activation='linear')(x) \n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "    \n",
    "model = transformer_regressor(\n",
    "    seq_len=seq_len, \n",
    "    n_features=n_features,\n",
    "    embed_dim=128,       # Or use your defined variables\n",
    "    ff_dim=256,\n",
    "    num_heads=8\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer='adam'           \n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "937fb3aa-4102-45c6-ba8e-4bd591657d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training regression model for EEM ---\n",
      "Epoch 1/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 350ms/step - loss: 0.8682 - mae: 0.6176 - val_loss: 0.0051 - val_mae: 0.0547\n",
      "Epoch 2/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 272ms/step - loss: 0.1316 - mae: 0.2888 - val_loss: 0.0040 - val_mae: 0.0460\n",
      "Epoch 3/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 216ms/step - loss: 0.0902 - mae: 0.2267 - val_loss: 0.0035 - val_mae: 0.0402\n",
      "Epoch 4/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 219ms/step - loss: 0.0959 - mae: 0.2436 - val_loss: 0.0096 - val_mae: 0.0920\n",
      "Epoch 5/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.1135 - mae: 0.2562 - val_loss: 7.7715e-04 - val_mae: 0.0200\n",
      "Epoch 6/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 222ms/step - loss: 0.0927 - mae: 0.2227 - val_loss: 8.6968e-04 - val_mae: 0.0245\n",
      "Epoch 7/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.0632 - mae: 0.1862 - val_loss: 6.8574e-04 - val_mae: 0.0215\n",
      "Epoch 8/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 220ms/step - loss: 0.0562 - mae: 0.1800 - val_loss: 0.0014 - val_mae: 0.0322\n",
      "Epoch 9/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 222ms/step - loss: 0.0740 - mae: 0.2046 - val_loss: 0.0141 - val_mae: 0.0903\n",
      "Epoch 10/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 219ms/step - loss: 0.0479 - mae: 0.1627 - val_loss: 0.0027 - val_mae: 0.0440\n",
      "Epoch 11/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 216ms/step - loss: 0.0583 - mae: 0.1817 - val_loss: 0.0010 - val_mae: 0.0277\n",
      "Epoch 12/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 222ms/step - loss: 0.0330 - mae: 0.1305 - val_loss: 0.0011 - val_mae: 0.0240\n",
      "Epoch 13/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 229ms/step - loss: 0.0370 - mae: 0.1462 - val_loss: 0.0020 - val_mae: 0.0391\n",
      "Epoch 14/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 260ms/step - loss: 0.0543 - mae: 0.1755 - val_loss: 7.7455e-04 - val_mae: 0.0220\n",
      "Epoch 15/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 218ms/step - loss: 0.0448 - mae: 0.1615 - val_loss: 6.1396e-04 - val_mae: 0.0198\n",
      "Epoch 16/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 220ms/step - loss: 0.0376 - mae: 0.1449 - val_loss: 4.7614e-04 - val_mae: 0.0167\n",
      "Epoch 17/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 220ms/step - loss: 0.0329 - mae: 0.1352 - val_loss: 0.0012 - val_mae: 0.0294\n",
      "Epoch 18/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 216ms/step - loss: 0.0352 - mae: 0.1451 - val_loss: 0.0028 - val_mae: 0.0481\n",
      "Epoch 19/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 220ms/step - loss: 0.0330 - mae: 0.1352 - val_loss: 3.7467e-04 - val_mae: 0.0136\n",
      "Epoch 20/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.0297 - mae: 0.1299 - val_loss: 0.0012 - val_mae: 0.0312\n",
      "Epoch 21/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 390ms/step - loss: 0.0267 - mae: 0.1246 - val_loss: 3.7096e-04 - val_mae: 0.0136\n",
      "Epoch 22/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 215ms/step - loss: 0.0244 - mae: 0.1179 - val_loss: 4.3584e-04 - val_mae: 0.0154\n",
      "Epoch 23/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 220ms/step - loss: 0.0234 - mae: 0.1167 - val_loss: 0.0026 - val_mae: 0.0459\n",
      "Epoch 24/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 216ms/step - loss: 0.0210 - mae: 0.1101 - val_loss: 4.0991e-04 - val_mae: 0.0149\n",
      "Epoch 25/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 216ms/step - loss: 0.0169 - mae: 0.0985 - val_loss: 0.0020 - val_mae: 0.0412\n",
      "Epoch 26/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 215ms/step - loss: 0.0156 - mae: 0.0963 - val_loss: 3.8872e-04 - val_mae: 0.0143\n",
      "Epoch 27/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 221ms/step - loss: 0.0124 - mae: 0.0864 - val_loss: 0.0010 - val_mae: 0.0283\n",
      "Epoch 28/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 256ms/step - loss: 0.0116 - mae: 0.0826 - val_loss: 4.2391e-04 - val_mae: 0.0150\n",
      "Epoch 29/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 218ms/step - loss: 0.0096 - mae: 0.0750 - val_loss: 4.8138e-04 - val_mae: 0.0174\n",
      "Epoch 30/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 222ms/step - loss: 0.0082 - mae: 0.0691 - val_loss: 3.4511e-04 - val_mae: 0.0132\n",
      "Epoch 31/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 370ms/step - loss: 0.0074 - mae: 0.0653 - val_loss: 2.8468e-04 - val_mae: 0.0110\n",
      "Epoch 32/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 225ms/step - loss: 0.0057 - mae: 0.0575 - val_loss: 3.1534e-04 - val_mae: 0.0122\n",
      "Epoch 33/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.0051 - mae: 0.0548 - val_loss: 2.8373e-04 - val_mae: 0.0109\n",
      "Epoch 34/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 324ms/step - loss: 0.0043 - mae: 0.0513 - val_loss: 7.1701e-04 - val_mae: 0.0229\n",
      "Epoch 35/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 238ms/step - loss: 0.0041 - mae: 0.0486 - val_loss: 6.0012e-04 - val_mae: 0.0205\n",
      "Epoch 36/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 231ms/step - loss: 0.0036 - mae: 0.0461 - val_loss: 4.4718e-04 - val_mae: 0.0152\n",
      "Epoch 37/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 227ms/step - loss: 0.0027 - mae: 0.0404 - val_loss: 3.7212e-04 - val_mae: 0.0136\n",
      "Epoch 38/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.0020 - mae: 0.0340 - val_loss: 3.8825e-04 - val_mae: 0.0142\n",
      "Epoch 39/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 220ms/step - loss: 0.0017 - mae: 0.0317 - val_loss: 3.0219e-04 - val_mae: 0.0115\n",
      "Epoch 40/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 214ms/step - loss: 0.0017 - mae: 0.0318 - val_loss: 2.8851e-04 - val_mae: 0.0112\n",
      "Epoch 41/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 215ms/step - loss: 0.0012 - mae: 0.0266 - val_loss: 3.2132e-04 - val_mae: 0.0119\n",
      "Epoch 42/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 218ms/step - loss: 0.0011 - mae: 0.0253 - val_loss: 2.8999e-04 - val_mae: 0.0112\n",
      "Epoch 43/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 9.0005e-04 - mae: 0.0227 - val_loss: 2.9774e-04 - val_mae: 0.0112\n",
      "\n",
      " Saving results for EEM \n",
      "Model saved to: regression_models/EEM_regressor_model.keras\n",
      "History saved to: regression_results/EEM_regressor_history.csv\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step\n",
      "Predictions saved to: regression_results/EEM_regressor_predictions.csv\n",
      "\n",
      "--- Evaluating EEM on Test Data ---\n",
      "Test Loss (MSE) for EEM: 0.000151\n",
      "Test Mean Absolute Error (MAE) for EEM: 0.009209\n",
      "--------------------------------------------------\n",
      "--- Training regression model for Gold ---\n",
      "Epoch 1/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 472ms/step - loss: 0.6391 - mae: 0.5724 - val_loss: 0.0151 - val_mae: 0.1042\n",
      "Epoch 2/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 240ms/step - loss: 0.1358 - mae: 0.2870 - val_loss: 4.4755e-04 - val_mae: 0.0155\n",
      "Epoch 3/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 237ms/step - loss: 0.1180 - mae: 0.2681 - val_loss: 0.0027 - val_mae: 0.0395\n",
      "Epoch 4/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 222ms/step - loss: 0.0977 - mae: 0.2415 - val_loss: 0.0016 - val_mae: 0.0329\n",
      "Epoch 5/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 230ms/step - loss: 0.0747 - mae: 0.2096 - val_loss: 0.0033 - val_mae: 0.0532\n",
      "Epoch 6/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 228ms/step - loss: 0.0755 - mae: 0.2110 - val_loss: 0.0022 - val_mae: 0.0422\n",
      "Epoch 7/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 301ms/step - loss: 0.0669 - mae: 0.2015 - val_loss: 8.5460e-04 - val_mae: 0.0236\n",
      "Epoch 8/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 223ms/step - loss: 0.0590 - mae: 0.1858 - val_loss: 0.0012 - val_mae: 0.0309\n",
      "Epoch 9/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 222ms/step - loss: 0.0665 - mae: 0.1962 - val_loss: 4.0999e-04 - val_mae: 0.0148\n",
      "Epoch 10/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 249ms/step - loss: 0.0660 - mae: 0.1966 - val_loss: 0.0051 - val_mae: 0.0693\n",
      "Epoch 11/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 220ms/step - loss: 0.0510 - mae: 0.1750 - val_loss: 0.0013 - val_mae: 0.0294\n",
      "Epoch 12/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 214ms/step - loss: 0.0413 - mae: 0.1562 - val_loss: 0.0035 - val_mae: 0.0540\n",
      "Epoch 13/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 219ms/step - loss: 0.0397 - mae: 0.1536 - val_loss: 0.0010 - val_mae: 0.0267\n",
      "Epoch 14/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 216ms/step - loss: 0.0373 - mae: 0.1486 - val_loss: 0.0014 - val_mae: 0.0314\n",
      "Epoch 15/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 215ms/step - loss: 0.0373 - mae: 0.1474 - val_loss: 3.6512e-04 - val_mae: 0.0151\n",
      "Epoch 16/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 274ms/step - loss: 0.0307 - mae: 0.1361 - val_loss: 2.9133e-04 - val_mae: 0.0122\n",
      "Epoch 17/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 288ms/step - loss: 0.0288 - mae: 0.1284 - val_loss: 7.3038e-04 - val_mae: 0.0243\n",
      "Epoch 18/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 264ms/step - loss: 0.0266 - mae: 0.1244 - val_loss: 3.1450e-04 - val_mae: 0.0132\n",
      "Epoch 19/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 239ms/step - loss: 0.0191 - mae: 0.1033 - val_loss: 7.0044e-04 - val_mae: 0.0235\n",
      "Epoch 20/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 339ms/step - loss: 0.0169 - mae: 0.0980 - val_loss: 4.4942e-04 - val_mae: 0.0172\n",
      "Epoch 21/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 318ms/step - loss: 0.0165 - mae: 0.0979 - val_loss: 3.6760e-04 - val_mae: 0.0165\n",
      "Epoch 22/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 229ms/step - loss: 0.0138 - mae: 0.0895 - val_loss: 4.3954e-04 - val_mae: 0.0184\n",
      "Epoch 23/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.0121 - mae: 0.0852 - val_loss: 3.1008e-04 - val_mae: 0.0145\n",
      "Epoch 24/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 222ms/step - loss: 0.0096 - mae: 0.0746 - val_loss: 3.8398e-04 - val_mae: 0.0149\n",
      "Epoch 25/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 216ms/step - loss: 0.0079 - mae: 0.0688 - val_loss: 4.8452e-04 - val_mae: 0.0174\n",
      "Epoch 26/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 223ms/step - loss: 0.0075 - mae: 0.0662 - val_loss: 0.0027 - val_mae: 0.0501\n",
      "\n",
      " Saving results for Gold \n",
      "Model saved to: regression_models/Gold_regressor_model.keras\n",
      "History saved to: regression_results/Gold_regressor_history.csv\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step\n",
      "Predictions saved to: regression_results/Gold_regressor_predictions.csv\n",
      "\n",
      "--- Evaluating Gold on Test Data ---\n",
      "Test Loss (MSE) for Gold: 0.000121\n",
      "Test Mean Absolute Error (MAE) for Gold: 0.008661\n",
      "--------------------------------------------------\n",
      "--- Training regression model for FTSE100 ---\n",
      "Epoch 1/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 469ms/step - loss: 1.2440 - mae: 0.7530 - val_loss: 0.0546 - val_mae: 0.2212\n",
      "Epoch 2/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 257ms/step - loss: 0.1774 - mae: 0.3283 - val_loss: 0.0204 - val_mae: 0.1345\n",
      "Epoch 3/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 216ms/step - loss: 0.1049 - mae: 0.2500 - val_loss: 9.8146e-04 - val_mae: 0.0271\n",
      "Epoch 4/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 239ms/step - loss: 0.0724 - mae: 0.2087 - val_loss: 0.0022 - val_mae: 0.0423\n",
      "Epoch 5/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 223ms/step - loss: 0.0534 - mae: 0.1777 - val_loss: 0.0098 - val_mae: 0.0948\n",
      "Epoch 6/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 263ms/step - loss: 0.0517 - mae: 0.1756 - val_loss: 7.2805e-04 - val_mae: 0.0227\n",
      "Epoch 7/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 277ms/step - loss: 0.0461 - mae: 0.1632 - val_loss: 0.0025 - val_mae: 0.0468\n",
      "Epoch 8/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 238ms/step - loss: 0.0408 - mae: 0.1548 - val_loss: 0.0045 - val_mae: 0.0650\n",
      "Epoch 9/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 235ms/step - loss: 0.0409 - mae: 0.1487 - val_loss: 3.4053e-04 - val_mae: 0.0128\n",
      "Epoch 10/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 248ms/step - loss: 0.0478 - mae: 0.1618 - val_loss: 0.0057 - val_mae: 0.0557\n",
      "Epoch 11/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 279ms/step - loss: 0.0510 - mae: 0.1617 - val_loss: 0.0019 - val_mae: 0.0330\n",
      "Epoch 12/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 225ms/step - loss: 0.0374 - mae: 0.1463 - val_loss: 0.0027 - val_mae: 0.0491\n",
      "Epoch 13/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 219ms/step - loss: 0.0355 - mae: 0.1405 - val_loss: 8.4234e-04 - val_mae: 0.0263\n",
      "Epoch 14/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 218ms/step - loss: 0.0356 - mae: 0.1430 - val_loss: 7.5978e-04 - val_mae: 0.0245\n",
      "Epoch 15/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 218ms/step - loss: 0.0330 - mae: 0.1362 - val_loss: 2.1454e-04 - val_mae: 0.0103\n",
      "Epoch 16/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 223ms/step - loss: 0.0296 - mae: 0.1336 - val_loss: 0.0025 - val_mae: 0.0451\n",
      "Epoch 17/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.0296 - mae: 0.1250 - val_loss: 2.2024e-04 - val_mae: 0.0100\n",
      "Epoch 18/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.0284 - mae: 0.1230 - val_loss: 5.6508e-04 - val_mae: 0.0209\n",
      "Epoch 19/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 216ms/step - loss: 0.0257 - mae: 0.1209 - val_loss: 0.0022 - val_mae: 0.0425\n",
      "Epoch 20/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 230ms/step - loss: 0.0259 - mae: 0.1216 - val_loss: 3.5465e-04 - val_mae: 0.0158\n",
      "Epoch 21/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 226ms/step - loss: 0.0253 - mae: 0.1182 - val_loss: 3.5808e-04 - val_mae: 0.0137\n",
      "Epoch 22/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 244ms/step - loss: 0.0251 - mae: 0.1194 - val_loss: 2.3147e-04 - val_mae: 0.0099\n",
      "Epoch 23/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 305ms/step - loss: 0.0265 - mae: 0.1208 - val_loss: 4.2295e-04 - val_mae: 0.0146\n",
      "Epoch 24/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 229ms/step - loss: 0.0202 - mae: 0.1069 - val_loss: 8.5013e-04 - val_mae: 0.0248\n",
      "Epoch 25/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 415ms/step - loss: 0.0212 - mae: 0.1093 - val_loss: 3.7351e-04 - val_mae: 0.0138\n",
      "\n",
      " Saving results for FTSE100 \n",
      "Model saved to: regression_models/FTSE100_regressor_model.keras\n",
      "History saved to: regression_results/FTSE100_regressor_history.csv\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step\n",
      "Predictions saved to: regression_results/FTSE100_regressor_predictions.csv\n",
      "\n",
      "--- Evaluating FTSE100 on Test Data ---\n",
      "Test Loss (MSE) for FTSE100: 0.000100\n",
      "Test Mean Absolute Error (MAE) for FTSE100: 0.007939\n",
      "--------------------------------------------------\n",
      "--- Training regression model for S&P500 ---\n",
      "Epoch 1/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 505ms/step - loss: 0.7888 - mae: 0.5932 - val_loss: 0.0389 - val_mae: 0.1827\n",
      "Epoch 2/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 261ms/step - loss: 0.1592 - mae: 0.3103 - val_loss: 0.0020 - val_mae: 0.0360\n",
      "Epoch 3/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 238ms/step - loss: 0.0902 - mae: 0.2317 - val_loss: 0.0016 - val_mae: 0.0314\n",
      "Epoch 4/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 236ms/step - loss: 0.1002 - mae: 0.2429 - val_loss: 0.0050 - val_mae: 0.0576\n",
      "Epoch 5/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 234ms/step - loss: 0.0963 - mae: 0.2405 - val_loss: 0.0121 - val_mae: 0.1048\n",
      "Epoch 6/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 247ms/step - loss: 0.0915 - mae: 0.2347 - val_loss: 0.0032 - val_mae: 0.0470\n",
      "Epoch 7/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 261ms/step - loss: 0.0831 - mae: 0.2151 - val_loss: 0.0038 - val_mae: 0.0573\n",
      "Epoch 8/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.0702 - mae: 0.1974 - val_loss: 0.0036 - val_mae: 0.0558\n",
      "Epoch 9/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 225ms/step - loss: 0.0714 - mae: 0.2062 - val_loss: 0.0030 - val_mae: 0.0509\n",
      "Epoch 10/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 215ms/step - loss: 0.0626 - mae: 0.1835 - val_loss: 0.0018 - val_mae: 0.0351\n",
      "Epoch 11/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 222ms/step - loss: 0.0433 - mae: 0.1550 - val_loss: 7.8096e-04 - val_mae: 0.0239\n",
      "Epoch 12/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 218ms/step - loss: 0.0575 - mae: 0.1834 - val_loss: 5.4940e-04 - val_mae: 0.0166\n",
      "Epoch 13/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 230ms/step - loss: 0.0339 - mae: 0.1406 - val_loss: 5.3261e-04 - val_mae: 0.0172\n",
      "Epoch 14/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 259ms/step - loss: 0.0323 - mae: 0.1361 - val_loss: 5.5499e-04 - val_mae: 0.0173\n",
      "Epoch 15/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 238ms/step - loss: 0.0298 - mae: 0.1318 - val_loss: 5.7221e-04 - val_mae: 0.0197\n",
      "Epoch 16/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 216ms/step - loss: 0.0288 - mae: 0.1261 - val_loss: 3.5239e-04 - val_mae: 0.0115\n",
      "Epoch 17/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 223ms/step - loss: 0.0222 - mae: 0.1123 - val_loss: 5.0557e-04 - val_mae: 0.0157\n",
      "Epoch 18/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 232ms/step - loss: 0.0194 - mae: 0.1062 - val_loss: 6.2791e-04 - val_mae: 0.0198\n",
      "Epoch 19/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 449ms/step - loss: 0.0195 - mae: 0.1060 - val_loss: 0.0022 - val_mae: 0.0404\n",
      "Epoch 20/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 509ms/step - loss: 0.0183 - mae: 0.1040 - val_loss: 0.0012 - val_mae: 0.0276\n",
      "Epoch 21/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 270ms/step - loss: 0.0218 - mae: 0.1120 - val_loss: 8.6367e-04 - val_mae: 0.0255\n",
      "Epoch 22/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 235ms/step - loss: 0.0183 - mae: 0.1031 - val_loss: 3.6468e-04 - val_mae: 0.0129\n",
      "Epoch 23/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 242ms/step - loss: 0.0189 - mae: 0.1062 - val_loss: 3.5455e-04 - val_mae: 0.0129\n",
      "Epoch 24/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 257ms/step - loss: 0.0145 - mae: 0.0916 - val_loss: 6.3859e-04 - val_mae: 0.0199\n",
      "Epoch 25/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 324ms/step - loss: 0.0175 - mae: 0.1005 - val_loss: 3.4791e-04 - val_mae: 0.0120\n",
      "Epoch 26/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 340ms/step - loss: 0.0138 - mae: 0.0901 - val_loss: 7.1118e-04 - val_mae: 0.0193\n",
      "Epoch 27/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 230ms/step - loss: 0.0130 - mae: 0.0893 - val_loss: 8.0213e-04 - val_mae: 0.0241\n",
      "Epoch 28/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 224ms/step - loss: 0.0130 - mae: 0.0891 - val_loss: 9.2580e-04 - val_mae: 0.0244\n",
      "Epoch 29/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 221ms/step - loss: 0.0120 - mae: 0.0848 - val_loss: 6.5389e-04 - val_mae: 0.0214\n",
      "Epoch 30/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.0107 - mae: 0.0802 - val_loss: 6.4599e-04 - val_mae: 0.0213\n",
      "Epoch 31/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 224ms/step - loss: 0.0097 - mae: 0.0770 - val_loss: 5.6958e-04 - val_mae: 0.0157\n",
      "Epoch 32/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.0099 - mae: 0.0768 - val_loss: 4.4455e-04 - val_mae: 0.0132\n",
      "Epoch 33/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 233ms/step - loss: 0.0088 - mae: 0.0719 - val_loss: 0.0010 - val_mae: 0.0245\n",
      "Epoch 34/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 219ms/step - loss: 0.0080 - mae: 0.0691 - val_loss: 5.5017e-04 - val_mae: 0.0148\n",
      "Epoch 35/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 246ms/step - loss: 0.0072 - mae: 0.0652 - val_loss: 4.3474e-04 - val_mae: 0.0124\n",
      "\n",
      " Saving results for S&P500 \n",
      "Model saved to: regression_models/S&P500_regressor_model.keras\n",
      "History saved to: regression_results/S&P500_regressor_history.csv\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 197ms/step\n",
      "Predictions saved to: regression_results/S&P500_regressor_predictions.csv\n",
      "\n",
      "--- Evaluating S&P500 on Test Data ---\n",
      "Test Loss (MSE) for S&P500: 0.000176\n",
      "Test Mean Absolute Error (MAE) for S&P500: 0.010233\n",
      "--------------------------------------------------\n",
      "--- Training regression model for Nikkei225 ---\n",
      "Epoch 1/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 465ms/step - loss: 0.6438 - mae: 0.5391 - val_loss: 0.0012 - val_mae: 0.0268\n",
      "Epoch 2/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 227ms/step - loss: 0.0766 - mae: 0.2118 - val_loss: 0.0034 - val_mae: 0.0464\n",
      "Epoch 3/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 240ms/step - loss: 0.0303 - mae: 0.1309 - val_loss: 8.7979e-04 - val_mae: 0.0239\n",
      "Epoch 4/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 229ms/step - loss: 0.0282 - mae: 0.1261 - val_loss: 0.0012 - val_mae: 0.0259\n",
      "Epoch 5/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 231ms/step - loss: 0.0321 - mae: 0.1359 - val_loss: 3.6376e-04 - val_mae: 0.0146\n",
      "Epoch 6/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 225ms/step - loss: 0.0486 - mae: 0.1669 - val_loss: 0.0029 - val_mae: 0.0485\n",
      "Epoch 7/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 254ms/step - loss: 0.0510 - mae: 0.1752 - val_loss: 3.6429e-04 - val_mae: 0.0150\n",
      "Epoch 8/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 230ms/step - loss: 0.0354 - mae: 0.1425 - val_loss: 4.9979e-04 - val_mae: 0.0171\n",
      "Epoch 9/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 225ms/step - loss: 0.0255 - mae: 0.1186 - val_loss: 3.9965e-04 - val_mae: 0.0156\n",
      "Epoch 10/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 291ms/step - loss: 0.0188 - mae: 0.1021 - val_loss: 3.3532e-04 - val_mae: 0.0143\n",
      "Epoch 11/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 258ms/step - loss: 0.0159 - mae: 0.0929 - val_loss: 4.4308e-04 - val_mae: 0.0165\n",
      "Epoch 12/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 231ms/step - loss: 0.0125 - mae: 0.0837 - val_loss: 3.6148e-04 - val_mae: 0.0154\n",
      "Epoch 13/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 236ms/step - loss: 0.0166 - mae: 0.0934 - val_loss: 8.9093e-04 - val_mae: 0.0210\n",
      "Epoch 14/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 220ms/step - loss: 0.0168 - mae: 0.0937 - val_loss: 4.4271e-04 - val_mae: 0.0166\n",
      "Epoch 15/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 234ms/step - loss: 0.0111 - mae: 0.0770 - val_loss: 5.8230e-04 - val_mae: 0.0203\n",
      "Epoch 16/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 224ms/step - loss: 0.0115 - mae: 0.0766 - val_loss: 6.5138e-04 - val_mae: 0.0206\n",
      "Epoch 17/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 227ms/step - loss: 0.0119 - mae: 0.0785 - val_loss: 2.8099e-04 - val_mae: 0.0127\n",
      "Epoch 18/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 222ms/step - loss: 0.0107 - mae: 0.0758 - val_loss: 3.0984e-04 - val_mae: 0.0137\n",
      "Epoch 19/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 225ms/step - loss: 0.0109 - mae: 0.0761 - val_loss: 2.7789e-04 - val_mae: 0.0122\n",
      "Epoch 20/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 265ms/step - loss: 0.0099 - mae: 0.0734 - val_loss: 2.3950e-04 - val_mae: 0.0115\n",
      "Epoch 21/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 238ms/step - loss: 0.0078 - mae: 0.0638 - val_loss: 3.8733e-04 - val_mae: 0.0158\n",
      "Epoch 22/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 219ms/step - loss: 0.0069 - mae: 0.0614 - val_loss: 2.0919e-04 - val_mae: 0.0105\n",
      "Epoch 23/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 230ms/step - loss: 0.0077 - mae: 0.0650 - val_loss: 2.7340e-04 - val_mae: 0.0124\n",
      "Epoch 24/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 221ms/step - loss: 0.0065 - mae: 0.0603 - val_loss: 5.1142e-04 - val_mae: 0.0191\n",
      "Epoch 25/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 230ms/step - loss: 0.0077 - mae: 0.0667 - val_loss: 3.0339e-04 - val_mae: 0.0130\n",
      "Epoch 26/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 237ms/step - loss: 0.0067 - mae: 0.0616 - val_loss: 2.2096e-04 - val_mae: 0.0109\n",
      "Epoch 27/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 315ms/step - loss: 0.0073 - mae: 0.0652 - val_loss: 2.9250e-04 - val_mae: 0.0133\n",
      "Epoch 28/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 222ms/step - loss: 0.0077 - mae: 0.0664 - val_loss: 5.0327e-04 - val_mae: 0.0185\n",
      "Epoch 29/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 221ms/step - loss: 0.0069 - mae: 0.0630 - val_loss: 4.7346e-04 - val_mae: 0.0186\n",
      "Epoch 30/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 0.0064 - mae: 0.0602 - val_loss: 1.9202e-04 - val_mae: 0.0098\n",
      "Epoch 31/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 257ms/step - loss: 0.0072 - mae: 0.0625 - val_loss: 1.9294e-04 - val_mae: 0.0098\n",
      "Epoch 32/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 324ms/step - loss: 0.0054 - mae: 0.0551 - val_loss: 2.6015e-04 - val_mae: 0.0124\n",
      "Epoch 33/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 294ms/step - loss: 0.0052 - mae: 0.0525 - val_loss: 3.4208e-04 - val_mae: 0.0149\n",
      "Epoch 34/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 226ms/step - loss: 0.0040 - mae: 0.0465 - val_loss: 1.9379e-04 - val_mae: 0.0097\n",
      "Epoch 35/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 282ms/step - loss: 0.0034 - mae: 0.0434 - val_loss: 2.0558e-04 - val_mae: 0.0102\n",
      "Epoch 36/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 215ms/step - loss: 0.0032 - mae: 0.0437 - val_loss: 2.7227e-04 - val_mae: 0.0124\n",
      "Epoch 37/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 289ms/step - loss: 0.0038 - mae: 0.0467 - val_loss: 3.1422e-04 - val_mae: 0.0140\n",
      "Epoch 38/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.0034 - mae: 0.0430 - val_loss: 2.0110e-04 - val_mae: 0.0098\n",
      "Epoch 39/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.0036 - mae: 0.0446 - val_loss: 2.0141e-04 - val_mae: 0.0100\n",
      "Epoch 40/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 216ms/step - loss: 0.0038 - mae: 0.0462 - val_loss: 2.0868e-04 - val_mae: 0.0100\n",
      "\n",
      " Saving results for Nikkei225 \n",
      "Model saved to: regression_models/Nikkei225_regressor_model.keras\n",
      "History saved to: regression_results/Nikkei225_regressor_history.csv\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 182ms/step\n",
      "Predictions saved to: regression_results/Nikkei225_regressor_predictions.csv\n",
      "\n",
      "--- Evaluating Nikkei225 on Test Data ---\n",
      "Test Loss (MSE) for Nikkei225: 0.000195\n",
      "Test Mean Absolute Error (MAE) for Nikkei225: 0.009804\n",
      "--------------------------------------------------\n",
      "--- Training regression model for UST10Y ---\n",
      "Epoch 1/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 545ms/step - loss: 1.0005 - mae: 0.6203 - val_loss: 0.0084 - val_mae: 0.0726\n",
      "Epoch 2/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 307ms/step - loss: 0.1078 - mae: 0.2579 - val_loss: 0.0051 - val_mae: 0.0541\n",
      "Epoch 3/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 227ms/step - loss: 0.1197 - mae: 0.2661 - val_loss: 0.0040 - val_mae: 0.0441\n",
      "Epoch 4/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 230ms/step - loss: 0.0732 - mae: 0.2057 - val_loss: 0.0072 - val_mae: 0.0593\n",
      "Epoch 5/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 238ms/step - loss: 0.0425 - mae: 0.1524 - val_loss: 0.0046 - val_mae: 0.0484\n",
      "Epoch 6/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 249ms/step - loss: 0.0168 - mae: 0.0982 - val_loss: 0.0038 - val_mae: 0.0425\n",
      "Epoch 7/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 361ms/step - loss: 0.0147 - mae: 0.0915 - val_loss: 0.0032 - val_mae: 0.0356\n",
      "Epoch 8/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 254ms/step - loss: 0.0151 - mae: 0.0915 - val_loss: 0.0033 - val_mae: 0.0373\n",
      "Epoch 9/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 274ms/step - loss: 0.0118 - mae: 0.0828 - val_loss: 0.0046 - val_mae: 0.0456\n",
      "Epoch 10/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 225ms/step - loss: 0.0126 - mae: 0.0848 - val_loss: 0.0034 - val_mae: 0.0385\n",
      "Epoch 11/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 225ms/step - loss: 0.0158 - mae: 0.0944 - val_loss: 0.0035 - val_mae: 0.0385\n",
      "Epoch 12/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 219ms/step - loss: 0.0257 - mae: 0.1173 - val_loss: 0.0046 - val_mae: 0.0474\n",
      "Epoch 13/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 223ms/step - loss: 0.0315 - mae: 0.1292 - val_loss: 0.0036 - val_mae: 0.0386\n",
      "Epoch 14/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 261ms/step - loss: 0.0352 - mae: 0.1346 - val_loss: 0.0052 - val_mae: 0.0512\n",
      "Epoch 15/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 225ms/step - loss: 0.0264 - mae: 0.1173 - val_loss: 0.0046 - val_mae: 0.0464\n",
      "Epoch 16/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 217ms/step - loss: 0.0149 - mae: 0.0890 - val_loss: 0.0102 - val_mae: 0.0798\n",
      "Epoch 17/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 222ms/step - loss: 0.0222 - mae: 0.1116 - val_loss: 0.0036 - val_mae: 0.0407\n",
      "\n",
      " Saving results for UST10Y \n",
      "Model saved to: regression_models/UST10Y_regressor_model.keras\n",
      "History saved to: regression_results/UST10Y_regressor_history.csv\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step\n",
      "Predictions saved to: regression_results/UST10Y_regressor_predictions.csv\n",
      "\n",
      "--- Evaluating UST10Y on Test Data ---\n",
      "Test Loss (MSE) for UST10Y: 0.000613\n",
      "Test Mean Absolute Error (MAE) for UST10Y: 0.019226\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "models_dir = \"regression_models\"\n",
    "results_dir = \"regression_results\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Initiating dicts to store models and history \n",
    "trained_models = {}\n",
    "history_dict = {}\n",
    "\n",
    "for asset in assets:\n",
    "    print(f\"--- Training regression model for {asset} ---\")\n",
    "\n",
    "    X_train_seq, X_test_seq = X_seq_dict[asset]\n",
    "    y_train_seq, y_test_seq = y_seq_dict[asset]\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = transformer_regressor(\n",
    "        seq_len=seq_len,\n",
    "        n_features=n_features,\n",
    "        embed_dim=embed_dim,\n",
    "        ff_dim=ff_dim,\n",
    "        num_heads=num_heads\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_seq,\n",
    "        y_train_seq,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2, \n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    trained_models[asset] = model\n",
    "    history_dict[asset] = history\n",
    "    \n",
    "    \n",
    "    print(f\"\\n Saving results for {asset} \")\n",
    "\n",
    "  \n",
    "    model_path = os.path.join(models_dir, f\"{asset}_regressor_model.keras\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "    \n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_path = os.path.join(results_dir, f\"{asset}_regressor_history.csv\")\n",
    "    history_df.to_csv(history_path)\n",
    "    print(f\"History saved to: {history_path}\")\n",
    "\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'actual_return': y_test_seq.flatten(),\n",
    "        'predicted_return': y_pred.flatten()\n",
    "    })\n",
    "    predictions_path = os.path.join(results_dir, f\"{asset}_regressor_predictions.csv\")\n",
    "    predictions_df.to_csv(predictions_path, index=False)\n",
    "    print(f\"Predictions saved to: {predictions_path}\")\n",
    "\n",
    "    print(f\"\\n--- Evaluating {asset} on Test Data ---\")\n",
    "    test_loss, test_mae = model.evaluate(X_test_seq, y_test_seq, verbose=0)\n",
    "    print(f\"Test Loss (MSE) for {asset}: {test_loss:.6f}\")\n",
    "    print(f\"Test Mean Absolute Error (MAE) for {asset}: {test_mae:.6f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1223c42d-c3fd-4a69-960c-39f0ccc88fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating Final Performance Metrics for All Assets ---\n",
      "\n",
      "--- Performance Metrics for: EEM ---\n",
      "Mean Squared Error (MSE):      0.000151\n",
      "Root Mean Squared Error (RMSE):  0.012292\n",
      "Mean Absolute Error (MAE):     0.009209\n",
      "\n",
      "--- Performance Metrics for: Gold ---\n",
      "Mean Squared Error (MSE):      0.000121\n",
      "Root Mean Squared Error (RMSE):  0.010981\n",
      "Mean Absolute Error (MAE):     0.008661\n",
      "\n",
      "--- Performance Metrics for: FTSE100 ---\n",
      "Mean Squared Error (MSE):      0.000100\n",
      "Root Mean Squared Error (RMSE):  0.010013\n",
      "Mean Absolute Error (MAE):     0.007939\n",
      "\n",
      "--- Performance Metrics for: S&P500 ---\n",
      "Mean Squared Error (MSE):      0.000176\n",
      "Root Mean Squared Error (RMSE):  0.013277\n",
      "Mean Absolute Error (MAE):     0.010233\n",
      "\n",
      "--- Performance Metrics for: Nikkei225 ---\n",
      "Mean Squared Error (MSE):      0.000195\n",
      "Root Mean Squared Error (RMSE):  0.013973\n",
      "Mean Absolute Error (MAE):     0.009804\n",
      "\n",
      "--- Performance Metrics for: UST10Y ---\n",
      "Mean Squared Error (MSE):      0.000613\n",
      "Root Mean Squared Error (RMSE):  0.024754\n",
      "Mean Absolute Error (MAE):     0.019226\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "\n",
    "# Define the folder where your results are stored\n",
    "results_dir = \"regression_results\"\n",
    "print(\"--- Calculating Final Performance Metrics for All Assets ---\")\n",
    "\n",
    "for asset in assets:\n",
    "    try:\n",
    "        # Construct the path to the prediction file\n",
    "        predictions_path = os.path.join(results_dir, f\"{asset}_regressor_predictions.csv\")\n",
    "        \n",
    "        # Check if the prediction file exists\n",
    "        if not os.path.exists(predictions_path):\n",
    "            print(f\"\\n--- SKIPPING: Prediction file for {asset} not found. ---\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Performance Metrics for: {asset} ---\")\n",
    "        df = pd.read_csv(predictions_path)\n",
    "\n",
    "        # Robustly clean the data to ensure it's numeric\n",
    "        df['actual_return'] = pd.to_numeric(df['actual_return'], errors='coerce')\n",
    "        df['predicted_return'] = pd.to_numeric(df['predicted_return'], errors='coerce')\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Check if the DataFrame is empty after cleaning\n",
    "        if df.empty:\n",
    "            print(\"WARNING: No valid data after cleaning. Cannot calculate metrics.\")\n",
    "            continue\n",
    "\n",
    "        # Extract the true and predicted values\n",
    "        y_true = df['actual_return']\n",
    "        y_pred = df['predicted_return']\n",
    "        \n",
    "        # Calculate and print performance metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "        print(f\"Mean Squared Error (MSE):      {mse:.6f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE):  {rmse:.6f}\")\n",
    "        print(f\"Mean Absolute Error (MAE):     {mae:.6f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # If any other error occurs, print it and move to the next asset\n",
    "        print(f\"--- FAILED TO PROCESS {asset}: An error occurred ---\")\n",
    "        print(f\"Error message: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1cf85-3384-435e-b697-4b8c0b4c515e",
   "metadata": {},
   "source": [
    "### Insights\n",
    "best Performing Models: The models for FTSE100 and Gold exhibit the lowest prediction error, with MAEs of 0.69% and 0.76%, respectively. These models provide the most reliable forecasts.\n",
    "Highest Error Model: The S&P500 model has the highest MAE at 2.46%, indicating its predictions are currently the least reliable.\n",
    "Asset Class Variation: There is significant performance variation across assets. The models appear to be more effective for assets that historically have different volatility profiles compared to major US equities.\n",
    "Error Magnitude: The Root Mean Squared Error (RMSE) is consistently higher than the MAE for all models. This is expected and indicates the presence of some larger prediction errors, which are penalized more heavily by the RMSE calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c8b19-bd7b-4bbe-a733-3911ac9847b8",
   "metadata": {},
   "source": [
    "### Transformer Regression Model: Improved Performance Metrics (Post-Tuning)\n",
    "\n",
    "The previous insights were with minimal tuning and after retuning hyperparameters and retraining, the regression models now provide better out-of-sample predictive performance, as measured by Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE):\n",
    "\n",
    "| Asset      | MSE      | RMSE    | MAE    |\n",
    "| :----------|--------: |-------: |------: |\n",
    "| **EEM**    | 0.000151 | 0.0123  | 0.0092 |\n",
    "| **Gold**   | 0.000121 | 0.0110  | 0.0087 |\n",
    "| **FTSE100**| 0.000100 | 0.0100  | 0.0079 |\n",
    "| **S&P500** | 0.000176 | 0.0133  | 0.0102 |\n",
    "| **Nikkei225** | 0.000195 | 0.0140 | 0.0098 |\n",
    "| **UST10Y** | 0.000613 | 0.0248  | 0.0192 |\n",
    "\n",
    "**Key observations:**\n",
    "- **FTSE100, Gold, and EEM models now achieve sub-1% average prediction error (MAE),** with FTSE100 being the most accurate.\n",
    "- **S&P500 and Nikkei225 show improvement with MAEs around 1%,** validating impact of hyperparameter tuning.\n",
    "- **UST10Y remains the hardest to predict (MAE: 1.92%),** possibly reflecting the higher unpredictability in bond yields.\n",
    "\n",
    "**Interpretation and Next Steps:**\n",
    "- The tighter range and improved accuracy should enable more meaningful, differentiated portfolio allocations—minimizing fallback to equal weighting.\n",
    "- Proceed to the portfolio management backtest and verify that allocations truly vary over time in response to model signals. Compare the Sharpe ratio and drawdown to earlier baseline runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533ab0c-83a7-4200-ac8d-bd4c7a20818a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qfai)",
   "language": "python",
   "name": "qfai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
