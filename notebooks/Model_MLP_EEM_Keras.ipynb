{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9bbdf42-81f9-4f69-9682-79c9bb3e28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22ee33ba-536d-4f36-8c3a-72d14eb6c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"../data/final_features_with_targets.csv\", parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "feature_cols = [col for col in final_df.columns if not col.endswith('_target_5d')]\n",
    "target_cols = [col for col in final_df.columns if col.endswith('_target_5d')]\n",
    "\n",
    "X = final_df[feature_cols]\n",
    "y = final_df[target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8377e478-b740-4559-ba89-20fdd01ad95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=(X_asset.shape[1],)))\n",
    "\n",
    "    # Tune number of hidden layers: 1 to 3\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f\"units_{i}\", min_value=32, max_value=256, step=32),\n",
    "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "        ))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    # Tune optimizer and learning rate\n",
    "    optimizer = hp.Choice(\"optimizer\", [\"adam\", \"rmsprop\", \"sgd\"])\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "236d7e38-d083-4a3e-8be8-6a5b56652185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from kt_mlp_tuning/EEM_MLP/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,                 # function previously created \n",
    "    objective='val_loss',        # Minimize validation loss\n",
    "    max_trials=10,               #  different combinations\n",
    "    executions_per_trial=2,      # Train each model twice and average the results\n",
    "    directory='kt_mlp_tuning',   # Folder to save search results\n",
    "    project_name='EEM_MLP'       # Only for one asset EEM, since it's easier to experiment on this \n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e425515f-2848-4131-977a-08123a32a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2962d71-cd53-47af-aec7-e90ee059e6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 3.3004 - mae: 1.3745 - val_loss: 0.0209 - val_mae: 0.1310\n",
      "Epoch 2/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0044 - mae: 0.0500 - val_loss: 0.0010 - val_mae: 0.0226\n",
      "Epoch 3/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 4.8782e-04 - mae: 0.0156 - val_loss: 4.1675e-04 - val_mae: 0.0142\n",
      "Epoch 4/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 3.3443e-04 - mae: 0.0135 - val_loss: 4.4374e-04 - val_mae: 0.0145\n",
      "Epoch 5/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2.9248e-04 - mae: 0.0125 - val_loss: 3.0892e-04 - val_mae: 0.0120\n",
      "Epoch 6/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2.4328e-04 - mae: 0.0117 - val_loss: 3.2910e-04 - val_mae: 0.0131\n",
      "Epoch 7/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2.9430e-04 - mae: 0.0132 - val_loss: 2.9061e-04 - val_mae: 0.0113\n",
      "Epoch 8/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 2.4574e-04 - mae: 0.0115 - val_loss: 3.4601e-04 - val_mae: 0.0137\n",
      "Epoch 9/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 3.1105e-04 - mae: 0.0137 - val_loss: 3.0700e-04 - val_mae: 0.0114\n",
      "Epoch 10/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.6354e-04 - mae: 0.0121 - val_loss: 0.0014 - val_mae: 0.0333\n",
      "Epoch 11/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 3.2760e-04 - mae: 0.0144 - val_loss: 8.6586e-04 - val_mae: 0.0262\n",
      "Epoch 12/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.3450e-04 - mae: 0.0143 - val_loss: 2.8628e-04 - val_mae: 0.0110\n",
      "Epoch 13/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 2.4184e-04 - mae: 0.0115 - val_loss: 2.9329e-04 - val_mae: 0.0112\n",
      "Epoch 14/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2.6455e-04 - mae: 0.0121 - val_loss: 0.0021 - val_mae: 0.0438\n",
      "Epoch 15/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 4.2302e-04 - mae: 0.0162 - val_loss: 3.0582e-04 - val_mae: 0.0122\n",
      "Epoch 16/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.4951e-04 - mae: 0.0145 - val_loss: 3.7861e-04 - val_mae: 0.0135\n",
      "Epoch 17/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2.8477e-04 - mae: 0.0130 - val_loss: 6.8172e-04 - val_mae: 0.0225\n",
      "Epoch 18/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 4.2623e-04 - mae: 0.0161 - val_loss: 4.4061e-04 - val_mae: 0.0154\n",
      "Epoch 19/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 4.2491e-04 - mae: 0.0163 - val_loss: 5.9608e-04 - val_mae: 0.0206\n",
      "Epoch 20/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.6087e-04 - mae: 0.0173 - val_loss: 0.0031 - val_mae: 0.0527\n",
      "Epoch 21/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 7.1793e-04 - mae: 0.0214 - val_loss: 5.8938e-04 - val_mae: 0.0191\n",
      "Epoch 22/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 6.1257e-04 - mae: 0.0194 - val_loss: 0.0028 - val_mae: 0.0509\n",
      "Epoch 23/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 6.2565e-04 - mae: 0.0202 - val_loss: 0.0015 - val_mae: 0.0357\n",
      "Epoch 24/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 6.5011e-04 - mae: 0.0201 - val_loss: 0.0019 - val_mae: 0.0402\n",
      "Epoch 25/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0272 - val_loss: 5.2839e-04 - val_mae: 0.0175\n",
      "Epoch 26/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 8.0078e-04 - mae: 0.0220 - val_loss: 0.0054 - val_mae: 0.0709\n",
      "Epoch 27/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0013 - mae: 0.0303 - val_loss: 0.0192 - val_mae: 0.1362\n",
      "Epoch 28/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.0106 - mae: 0.0863 - val_loss: 3.1553e-04 - val_mae: 0.0114\n",
      "Epoch 29/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0031 - mae: 0.0429 - val_loss: 0.0130 - val_mae: 0.1117\n",
      "Epoch 30/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0036 - mae: 0.0488 - val_loss: 0.0569 - val_mae: 0.2358\n",
      "Epoch 31/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0180 - mae: 0.1027 - val_loss: 0.0169 - val_mae: 0.1274\n",
      "Epoch 32/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0064 - mae: 0.0663 - val_loss: 0.0020 - val_mae: 0.0418\n",
      "Epoch 33/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0063 - mae: 0.0615 - val_loss: 0.0676 - val_mae: 0.2563\n",
      "Epoch 34/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0275 - mae: 0.1418 - val_loss: 0.0017 - val_mae: 0.0380\n",
      "Epoch 35/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0036 - mae: 0.0500 - val_loss: 6.8659e-04 - val_mae: 0.0226\n",
      "Epoch 36/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0070 - mae: 0.0665 - val_loss: 0.0028 - val_mae: 0.0498\n",
      "Epoch 37/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0203 - mae: 0.0955 - val_loss: 0.0363 - val_mae: 0.1883\n",
      "Epoch 38/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0065 - mae: 0.0650 - val_loss: 9.0970e-04 - val_mae: 0.0270\n",
      "Epoch 39/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0012 - mae: 0.0290 - val_loss: 3.2428e-04 - val_mae: 0.0120\n",
      "Epoch 40/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 5.6601e-04 - mae: 0.0175 - val_loss: 5.0503e-04 - val_mae: 0.0182\n",
      "Epoch 41/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0022 - mae: 0.0371 - val_loss: 3.0133e-04 - val_mae: 0.0112\n",
      "Epoch 42/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0019 - mae: 0.0351 - val_loss: 9.6171e-04 - val_mae: 0.0279\n",
      "Epoch 43/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0597 - mae: 0.1283 - val_loss: 0.1455 - val_mae: 0.3755\n",
      "Epoch 44/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0323 - mae: 0.1329 - val_loss: 6.9852e-04 - val_mae: 0.0180\n",
      "Epoch 45/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0080 - mae: 0.0696 - val_loss: 4.6227e-04 - val_mae: 0.0172\n",
      "Epoch 46/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 3.5139e-04 - mae: 0.0141 - val_loss: 4.1410e-04 - val_mae: 0.0159\n",
      "Epoch 47/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.9732e-04 - mae: 0.0131 - val_loss: 8.0522e-04 - val_mae: 0.0235\n",
      "Epoch 48/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.5009e-04 - mae: 0.0139 - val_loss: 0.0018 - val_mae: 0.0400\n",
      "Epoch 49/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.6751e-04 - mae: 0.0171 - val_loss: 7.4140e-04 - val_mae: 0.0223\n",
      "Epoch 50/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.8347e-04 - mae: 0.0155 - val_loss: 4.0138e-04 - val_mae: 0.0155\n",
      "Epoch 51/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.3318e-04 - mae: 0.0144 - val_loss: 5.7728e-04 - val_mae: 0.0187\n",
      "Epoch 52/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 5.9992e-04 - mae: 0.0197 - val_loss: 7.7853e-04 - val_mae: 0.0231\n",
      "Epoch 53/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 3.5380e-04 - mae: 0.0145 - val_loss: 0.0017 - val_mae: 0.0395\n",
      "Epoch 54/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 8.8182e-04 - mae: 0.0248 - val_loss: 0.0021 - val_mae: 0.0426\n",
      "Epoch 55/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 7.5722e-04 - mae: 0.0224 - val_loss: 5.6262e-04 - val_mae: 0.0198\n",
      "Epoch 56/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 4.0987e-04 - mae: 0.0156 - val_loss: 2.8923e-04 - val_mae: 0.0111\n",
      "Epoch 57/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 5.9546e-04 - mae: 0.0189 - val_loss: 3.3074e-04 - val_mae: 0.0129\n",
      "Epoch 58/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 9.3848e-04 - mae: 0.0244 - val_loss: 4.9318e-04 - val_mae: 0.0180\n",
      "Epoch 59/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 4.7360e-04 - mae: 0.0170 - val_loss: 5.4787e-04 - val_mae: 0.0194\n",
      "Epoch 60/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 4.7824e-04 - mae: 0.0172 - val_loss: 0.0022 - val_mae: 0.0433\n",
      "Epoch 61/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 7.6208e-04 - mae: 0.0228 - val_loss: 6.9643e-04 - val_mae: 0.0229\n",
      "Epoch 62/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0268 - val_loss: 0.0017 - val_mae: 0.0376\n",
      "Epoch 63/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 5.3938e-04 - mae: 0.0186 - val_loss: 0.0049 - val_mae: 0.0674\n",
      "Epoch 64/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0017 - mae: 0.0347 - val_loss: 0.0028 - val_mae: 0.0499\n",
      "Epoch 65/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0017 - mae: 0.0333 - val_loss: 0.0012 - val_mae: 0.0306\n",
      "Epoch 66/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0028 - mae: 0.0413 - val_loss: 0.0173 - val_mae: 0.1292\n",
      "Epoch 67/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0021 - mae: 0.0376 - val_loss: 0.0049 - val_mae: 0.0681\n",
      "Epoch 68/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0040 - mae: 0.0538 - val_loss: 4.4425e-04 - val_mae: 0.0167\n",
      "Epoch 69/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0059 - val_mae: 0.0742\n",
      "Epoch 70/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0024 - mae: 0.0402 - val_loss: 0.0059 - val_mae: 0.0740\n",
      "Epoch 71/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0041 - mae: 0.0526 - val_loss: 0.0041 - val_mae: 0.0622\n",
      "Epoch 72/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 8.8475e-04 - mae: 0.0241 - val_loss: 0.0050 - val_mae: 0.0681\n",
      "Epoch 73/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.0040 - mae: 0.0532 - val_loss: 2.9017e-04 - val_mae: 0.0111\n",
      "Epoch 74/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 9.1232e-04 - mae: 0.0241 - val_loss: 0.0023 - val_mae: 0.0459\n",
      "Epoch 75/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0016 - mae: 0.0330 - val_loss: 3.1510e-04 - val_mae: 0.0126\n",
      "Epoch 76/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0025 - mae: 0.0423 - val_loss: 8.4714e-04 - val_mae: 0.0251\n",
      "Epoch 77/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0138 - mae: 0.0930 - val_loss: 3.2695e-04 - val_mae: 0.0119\n",
      "Epoch 78/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 8.0183e-04 - mae: 0.0232 - val_loss: 0.0021 - val_mae: 0.0430\n",
      "Epoch 79/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0010 - mae: 0.0268 - val_loss: 0.0105 - val_mae: 0.1005\n",
      "Epoch 80/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0025 - mae: 0.0423 - val_loss: 0.0013 - val_mae: 0.0329\n",
      "Epoch 81/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0017 - mae: 0.0281 - val_loss: 0.0010 - val_mae: 0.0292\n",
      "Epoch 82/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0014 - mae: 0.0292 - val_loss: 0.1087 - val_mae: 0.3261\n",
      "Epoch 83/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0173 - mae: 0.1023 - val_loss: 0.0044 - val_mae: 0.0642\n",
      "Epoch 84/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0252 - val_loss: 0.0027 - val_mae: 0.0488\n",
      "Epoch 85/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 6.4330e-04 - mae: 0.0203 - val_loss: 0.0041 - val_mae: 0.0614\n",
      "Epoch 86/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0039 - mae: 0.0536 - val_loss: 0.0021 - val_mae: 0.0433\n",
      "Epoch 87/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0045 - mae: 0.0572 - val_loss: 3.1750e-04 - val_mae: 0.0119\n",
      "Epoch 88/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0020 - mae: 0.0357 - val_loss: 0.0019 - val_mae: 0.0413\n",
      "Epoch 89/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0017 - mae: 0.0344 - val_loss: 0.0110 - val_mae: 0.1026\n",
      "Epoch 90/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0059 - mae: 0.0628 - val_loss: 0.0021 - val_mae: 0.0424\n",
      "Epoch 91/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 9.7071e-04 - mae: 0.0244 - val_loss: 0.0014 - val_mae: 0.0333\n",
      "Epoch 92/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0041 - mae: 0.0548 - val_loss: 6.6592e-04 - val_mae: 0.0208\n",
      "Epoch 93/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0037 - mae: 0.0482 - val_loss: 3.5994e-04 - val_mae: 0.0141\n",
      "Epoch 94/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0022 - mae: 0.0346 - val_loss: 0.0027 - val_mae: 0.0488\n",
      "Epoch 95/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0014 - val_mae: 0.0348\n",
      "Epoch 96/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0010 - mae: 0.0257 - val_loss: 2.8355e-04 - val_mae: 0.0109\n",
      "Epoch 97/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0014 - mae: 0.0300 - val_loss: 3.7955e-04 - val_mae: 0.0148\n",
      "Epoch 98/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 9.4408e-04 - mae: 0.0216 - val_loss: 0.0345 - val_mae: 0.1833\n",
      "Epoch 99/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0069 - mae: 0.0661 - val_loss: 0.0028 - val_mae: 0.0487\n",
      "Epoch 100/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0036 - mae: 0.0444 - val_loss: 0.0022 - val_mae: 0.0449\n"
     ]
    }
   ],
   "source": [
    "#We build the model with the best hyper parameter we found in ther previously \n",
    "model = build_model(best_hp)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198e97e-749b-4a43-a136-102b71e8c0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead1539-9e7a-46e8-a971-3e223f82a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction \n",
    "y_pred = model.predict(X_val)\n",
    "#Evaluation \n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE : {mae:.5f}\")\n",
    "print(f\"R²  : {r2:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26837eb5-40a2-427e-af6a-620c1e1a448f",
   "metadata": {},
   "source": [
    "#### Some insights and Comments \n",
    "We tuned MLP on just one asset EEM the following were the results\n",
    "Evaluation Results\n",
    "RMSE = 0.04231 | MAE = 0.04046 | R² = -10.93108\n",
    "\n",
    "Even after hyperparameter tuning using Keras Tuner, the MLP model performs poorly on EEM with a highly negative R² score.\n",
    "This suggests the model fails to capture meaningful return dynamics or overfits during training.\n",
    "We'll consider regularization or early stopping later, but for now, we move to other models.We will do one batch run for all the assets and see if this pattern follows and then we move to the LSTM and transformer models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5ad5a-d995-4f13-9ea1-604f380f68dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qfai)",
   "language": "python",
   "name": "qfai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
